{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83fad96-98dc-4368-a45b-422028b33996",
   "metadata": {},
   "source": [
    "需求分析在百度贴吧中任意寻找一个贴吧并打开一个热门帖子，将帖子的源代码复制下来，并保存为source.txt。Python读入这个source.txt文件，通过正则表达式获取用户名、发帖内容和发帖时间，并保存为result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d65fd4-e672-4569-86d4-f9551d66e821",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m reply_time_list \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail-info\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>(.*?)<\u001b[39m\u001b[38;5;124m'\u001b[39m, source, re\u001b[38;5;241m.\u001b[39mS)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(username_list)):\n\u001b[1;32m---> 13\u001b[0m     result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m: username_list[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mcontent_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreply_time\u001b[39m\u001b[38;5;124m'\u001b[39m: reply_time_list[i]}\n\u001b[0;32m     14\u001b[0m     result_list\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m第一个爬虫例子result.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "with open('第一个爬虫例子source.txt', 'r', encoding='utf-8') as f:\n",
    "    source = f.read()\n",
    "    \n",
    "result_list = []\n",
    "username_list = re.findall('username=\"(.*?)\"', source, re.S)\n",
    "content_list = re.findall('j_d_post_content \">(.*?)<', source, re.S)\n",
    "reply_time_list = re.findall('class=\"tail-info\">(.*?)<', source, re.S)\n",
    "\n",
    "for i in range(len(username_list)):\n",
    "    result = {'username': username_list[i], 'content': content_list[i], 'reply_time': reply_time_list[i]}\n",
    "    result_list.append(result)\n",
    "    \n",
    "with open('第一个爬虫例子result.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['username', 'content', 'reply_time'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97dc950a-a727-4dcc-8f27-88b8b20eb780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本块的数量：15\n",
      "每一块的内容:{'username': '一贱惊鸿十九州', 'content': '没有质疑是不是刷的，不过抖亿播放都很少见的，B不是看到人更少吗？', 'reply_time': '2024-02-15 20:29'}\n",
      "每一块的内容:{'username': 'ichu永远永远', 'content': '我觉得最多就是买流量了', 'reply_time': '2024-02-15 20:31'}\n",
      "每一块的内容:{'username': '', 'content': '没看过，不清楚', 'reply_time': '2024-02-15 20:53'}\n",
      "每一块的内容:{'username': 'qtrlmusic', 'content': 'B站总共好像也就几千万用户吧', 'reply_time': '2024-02-15 21:02'}\n",
      "每一块的内容:{'username': 'sfu525', 'content': '电棍硅胶每天都有人反复刷，不奇怪', 'reply_time': '2024-02-17 09:01'}\n",
      "每一块的内容:{'username': 'zsw56210856', 'content': '抖可能是分流多吧，抖的tag可是动不动几亿十几亿播放来着', 'reply_time': '2024-02-17 13:13'}\n",
      "每一块的内容:{'username': '红针花瓣', 'content': '1.让子弹飞本身就有话题度，容易引流；', 'reply_time': '2024-02-17 13:18'}\n",
      "每一块的内容:{'username': '坑爹老坑', 'content': '有一说一还挺好听的。', 'reply_time': '2024-02-17 20:33'}\n",
      "每一块的内容:{'username': '许玕顺德容桂', 'content': '调的可以配上它现在的播放量，但是现在还有3000+我觉得有买的嫌疑，或者是最近b站热搜带的热度也说不定', 'reply_time': '2024-02-17 20:50'}\n",
      "每一块的内容:{'username': '叶子打比啦', 'content': '反复看 正常 这个视频在相关视频下面都有推送 刷着刷着又点回去看一遍了', 'reply_time': '2024-02-17 23:21'}\n",
      "每一块的内容:{'username': 'Q422926885', 'content': '这个最火时一堆人开着洗脑循环放的', 'reply_time': '2024-02-18 01:37'}\n",
      "每一块的内容:{'username': '核弹黄人逊', 'content': '罗翔3000w了', 'reply_time': '2024-02-18 01:39'}\n",
      "每一块的内容:{'username': '鲶笠矢的尾巴呢', 'content': '有些视频播放量高是推送的原因，你刷来刷去还是会回到这个视频，索性就点进去重温了', 'reply_time': '2024-02-18 06:54'}\n",
      "每一块的内容:{'username': '12346亲亲亲', 'content': '太好看了', 'reply_time': '2024-02-18 07:36'}\n",
      "每一块的内容:{'username': '39民国', 'content': '挺好看的', 'reply_time': '2024-02-18 10:11'}\n"
     ]
    }
   ],
   "source": [
    "with open('第一个爬虫例子source.txt', 'r', encoding='utf-8') as f:\n",
    "    source = f.read() \n",
    "\n",
    "# 逻辑上更合理的代码\n",
    "# 首先获得包含每一层楼所有信息的大文本快\n",
    "every_reply = re.findall('l_post l_post_bright j_l_post clearfix  \"(.*?)p_props_tail props_appraise_wrap', source, re.S)\n",
    "print('文本块的数量：{}'.format(len(every_reply)))\n",
    "\n",
    "result_list = []\n",
    "\n",
    "# 从每一个大文本块里面提取出各个楼层的发帖人姓名、发帖内容和发帖时间\n",
    "for each in every_reply: \n",
    "    result = {}\n",
    "    result['username'] = re.findall('username=\"(.*?)\"', each, re.S)[0]\n",
    "    result['content'] = re.findall('class=\"d_post_content j_d_post_content \" style=\"display:;\">                    (.*?)<', each, re.S)[0]\n",
    "    result['reply_time'] = re.findall('class=\"tail-info\">(2024.*?)<', each, re.S)[0]\n",
    "    print('每一块的内容:{}'.format(result))\n",
    "    result_list.append(result)\n",
    "    \n",
    "with open('第一个爬虫例子result.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['username', 'content', 'reply_time'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(result_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
